{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1536204",
   "metadata": {},
   "source": [
    "## LIBRERIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c04263a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import pyodbc\n",
    "from fake_useragent import UserAgent\n",
    "import random\n",
    "from urllib.parse import unquote, urlparse, parse_qs\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f177e9",
   "metadata": {},
   "source": [
    "### 1. Configuración Inicial\n",
    "Definición de User-Agents, parametros de conección a la base de datos, configuración para el multithreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7dba147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuración avanzada\n",
    "ua = UserAgent(\n",
    "    browsers=['chrome', 'firefox', 'safari'],  # Navegadores comunes\n",
    "    platforms=['windows', 'macos', 'linux'],   # Sistemas operativos\n",
    "    min_percentage=1.5  # Solo agentes con al menos 1.5% de uso global\n",
    ")\n",
    "\n",
    "\n",
    "# Configuración de SQL Server\n",
    "SERVER = 'TuServidor'  # Ejemplo: 'LAPTOP-ABC123\\SQLEXPRESS' o 'localhost'\n",
    "DATABASE = 'TuBaseDeDatos'  # Nombre de tu base de datos\n",
    "USERNAME = 'TuUsername'     # Dejar en blanco si usas autenticación de Windows\n",
    "PASSWORD = 'TuPassword'  # Dejar en blanco si usas autenticación de Windows\n",
    "TRUSTED_CONNECTION = 'yes'  # Usar 'yes' para autenticación de Windows, 'no' para SQL auth\n",
    "\n",
    "# Configuración de threading\n",
    "MAX_WORKERS = 4  # Número de hilos concurrentes (ajustar según tu conexión)\n",
    "REQUEST_DELAY = (1, 2)  # Delay entre requests en segundos (min, max)\n",
    "\n",
    "# Lock para thread-safe operations\n",
    "print_lock = threading.Lock()\n",
    "stats_lock = threading.Lock()\n",
    "\n",
    "# Estadísticas globales\n",
    "global_stats = {\n",
    "    'pages_processed': 0,\n",
    "    'products_found': 0,\n",
    "    'errors': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce97929",
   "metadata": {},
   "source": [
    "### 2. Funciones para la extracción de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d01054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_safe_print(message):\n",
    "    \"\"\"Print thread-safe con lock\"\"\"\n",
    "    with print_lock:\n",
    "        print(message)\n",
    "\n",
    "def update_stats(pages=0, products=0, errors=0):\n",
    "    \"\"\"Actualiza estadísticas de forma thread-safe\"\"\"\n",
    "    with stats_lock:\n",
    "        global_stats['pages_processed'] += pages\n",
    "        global_stats['products_found'] += products\n",
    "        global_stats['errors'] += errors\n",
    "\n",
    "def get_random_headers():\n",
    "    return {\n",
    "        'User-Agent': ua.random,\n",
    "        'Accept-Language': 'es-ES,es;q=0.9',\n",
    "        'Referer': random.choice([\n",
    "            'https://www.google.com/',\n",
    "            'https://www.mercadolibre.com.ar/',\n",
    "            'https://www.bing.com/'\n",
    "        ]),\n",
    "        'Sec-Fetch-Dest': 'document',\n",
    "        'Sec-Fetch-Mode': 'navigate',\n",
    "        'DNT': str(random.randint(1, 2))\n",
    "    }\n",
    "\n",
    "\n",
    "def clean_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia y acorta las URLs de MercadoLibre eliminando parámetros de tracking.\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        # Si es una URL de click tracking, extraer la URL real\n",
    "        if 'mclics/clicks' in url:\n",
    "            # Buscar el parámetro que contiene la URL real\n",
    "            parsed = urlparse(url)\n",
    "            query_params = parse_qs(parsed.query)\n",
    "            \n",
    "            # Intentar extraer la URL real de los parámetros\n",
    "            for param_name, param_values in query_params.items():\n",
    "                if param_values and 'mercadolibre.com.ar' in str(param_values[0]):\n",
    "                    return unquote(param_values[0])\n",
    "        \n",
    "        # Si no es URL de tracking, limpiar parámetros innecesarios\n",
    "        parsed = urlparse(url)\n",
    "        clean_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "        \n",
    "        # Mantener solo parámetros esenciales\n",
    "        if parsed.query:\n",
    "            query_params = parse_qs(parsed.query)\n",
    "            essential_params = {}\n",
    "            for key in ['id', 'item_id', 'category_id']:\n",
    "                if key in query_params:\n",
    "                    essential_params[key] = query_params[key][0]\n",
    "            \n",
    "            if essential_params:\n",
    "                from urllib.parse import urlencode\n",
    "                clean_url += '?' + urlencode(essential_params)\n",
    "        \n",
    "        return clean_url\n",
    "    except Exception:\n",
    "        # Si hay error, devolver la URL original pero truncada\n",
    "        return url[:800] if len(url) > 800 else url\n",
    "    \n",
    "\n",
    "def clean_mercadolibre_search_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    Limpia las URLs de búsqueda de MercadoLibre eliminando parámetros y fragmentos.\n",
    "    \n",
    "    Ejemplos:\n",
    "    - Input:  https://listado.mercadolibre.com.ar/colchon-inflable?sb=all_mercadolibre#D[A:colchon%20inflable]\n",
    "    - Output: https://listado.mercadolibre.com.ar/colchon-inflable\n",
    "    \n",
    "    - Input:  https://listado.mercadolibre.com.ar/pantalon-cargo#D[A:pantalon%20cargo]\n",
    "    - Output: https://listado.mercadolibre.com.ar/pantalon-cargo\n",
    "    \n",
    "    Args:\n",
    "        url (str): URL de MercadoLibre con parámetros y fragmentos\n",
    "        \n",
    "    Returns:\n",
    "        str: URL limpia sin parámetros ni fragmentos\n",
    "    \"\"\"\n",
    "    if not url:\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        # Parsear la URL\n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        # Construir URL limpia: scheme + netloc + path (sin query ni fragment)\n",
    "        clean_url = f\"{parsed.scheme}://{parsed.netloc}{parsed.path}\"\n",
    "        \n",
    "        return clean_url\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error al limpiar URL: {e}\")\n",
    "        return url  # Devolver URL original si hay error\n",
    "\n",
    "\n",
    "def extract_product_image(item):\n",
    "    # Suponiendo que `item` es el contenedor del producto\n",
    "    img_tag = item.find('img', class_='poly-component__picture')\n",
    "\n",
    "    # 1) Revisar data-src o data-original\n",
    "    img_url = None\n",
    "    if img_tag:\n",
    "        img_url = (\n",
    "            img_tag.get('data-src') or\n",
    "            img_tag.get('data-original') or\n",
    "            img_tag.get('data-lazy')  # a veces usan este nombre\n",
    "        )\n",
    "\n",
    "    # 2) Si usan srcset, tomar la primera URL\n",
    "    if not img_url and img_tag and img_tag.get('srcset'):\n",
    "        # srcset suele ser algo como \"https://...-E.webp 1x, https://...-F.webp 2x\"\n",
    "        srcset = img_tag['srcset']\n",
    "        # Partimos por comas y luego espacio antes de la densidad\n",
    "        img_url = srcset.split(',')[0].split()[0]\n",
    "\n",
    "    # 3) Fallback a src (no recomendado, es el placeholder)\n",
    "    if not img_url and img_tag:\n",
    "        img_url = img_tag.get('src')\n",
    "\n",
    "    return img_url\n",
    "\n",
    "def scrape_single_page(page_info: dict) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Función optimizada para scraping de una sola página - thread-safe.\n",
    "    Recibe un diccionario con 'url', 'page_num' y 'thread_id'\n",
    "    \"\"\"\n",
    "    page_url = page_info['url']\n",
    "    page_num = page_info['page_num']\n",
    "    thread_id = page_info['thread_id']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        # Delay aleatorio para evitar sobrecargar el servidor\n",
    "        delay = random.uniform(*REQUEST_DELAY)\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        thread_safe_print(f\"🔄 [Hilo {thread_id}] Procesando página {page_num}: {page_url}\")\n",
    "        \n",
    "        # Realizar request con headers aleatorios\n",
    "        session = requests.Session()\n",
    "        response = session.get(page_url, headers=get_random_headers(), timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        items = soup.find_all('li', class_='ui-search-layout__item')\n",
    "        \n",
    "        if not items:\n",
    "            thread_safe_print(f\"⚠️  [Hilo {thread_id}] No se encontraron productos en página {page_num}\")\n",
    "            update_stats(pages=1, errors=1)\n",
    "            return results\n",
    "        \n",
    "        for item in items:\n",
    "            try:\n",
    "                # Nombre del producto\n",
    "                name_element = item.find('h3', class_='poly-component__title-wrapper')\n",
    "                name = name_element.text.strip() if name_element else 'Sin nombre'\n",
    "\n",
    "                # Precio del producto\n",
    "                price_element = item.find('span', class_='andes-money-amount andes-money-amount--cents-superscript')\n",
    "                if price_element:\n",
    "                    price_text = price_element.text.replace('$', '').replace('.', '').replace(',', '')\n",
    "                    price_clean = int(re.sub(r'[^\\d]', '', price_text)) if price_text else 0\n",
    "                else:\n",
    "                    price_clean = 0\n",
    "                \n",
    "                # Link\n",
    "                link_element = item.find(\"a\", class_='poly-component__title', href=True)\n",
    "                link = link_element[\"href\"] if link_element else ''\n",
    "                \n",
    "                # Ventas\n",
    "                try:\n",
    "                    sales_element = item.find('span', class_='poly-reviews__total')\n",
    "                    sales = sales_element.text.strip().replace('(', '').replace(')', '') if sales_element else '0'\n",
    "                    sales_clean = int(re.sub(r'[^\\d]', '', sales)) if sales else 0\n",
    "                except (AttributeError, ValueError):\n",
    "                    sales_clean = 0\n",
    "                    \n",
    "                # Calificación\n",
    "                try:\n",
    "                    rate_element = item.find('span', class_='poly-reviews__rating')\n",
    "                    rate = rate_element.text.strip() if rate_element else '0'\n",
    "                    rate_clean = float(rate.replace(',', '.')) if rate else 0.0\n",
    "                except (AttributeError, ValueError):\n",
    "                    rate_clean = 0.0\n",
    "                \n",
    "                results.append({\n",
    "                    'url': clean_url(link),\n",
    "                    'nombre': name,\n",
    "                    'precio': price_clean,\n",
    "                    'ventas': sales_clean,\n",
    "                    'calificacion': rate_clean,\n",
    "                    'imagen': extract_product_image(item)\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                thread_safe_print(f\"❌ [Hilo {thread_id}] Error procesando producto: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Actualizar estadísticas\n",
    "        update_stats(pages=1, products=len(results))\n",
    "        thread_safe_print(f\"✅ [Hilo {thread_id}] Página {page_num} completada: {len(results)} productos\")\n",
    "        \n",
    "        session.close()\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        thread_safe_print(f\"❌ [Hilo {thread_id}] Error de red en página {page_num}: {e}\")\n",
    "        update_stats(pages=1, errors=1)\n",
    "    except Exception as e:\n",
    "        thread_safe_print(f\"❌ [Hilo {thread_id}] Error general en página {page_num}: {e}\")\n",
    "        update_stats(pages=1, errors=1)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_all_products_threaded(base_url: str, max_pages: int = 10, max_workers: int = MAX_WORKERS) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Version con multihilos para extraer productos de múltiples páginas concurrentemente.\n",
    "    CORREGIDA para manejar correctamente la paginación de MercadoLibre.\n",
    "    \"\"\"\n",
    "    thread_safe_print(f\"🚀 Iniciando scraping multihilo con {max_workers} workers\")\n",
    "    \n",
    "    # URL base para páginas 2 en adelante (diferente a la primera página)\n",
    "    #PAGINATED_BASE_URL = \"https://listado.mercadolibre.com.ar/campera-polar\"\n",
    "    url_clean=clean_mercadolibre_search_url(base_url)\n",
    "    \n",
    "    # Preparar lista de páginas para procesar\n",
    "    page_tasks = []\n",
    "    for i in range(max_pages):\n",
    "        if i == 0:\n",
    "            # Primera página: usar la URL original\n",
    "            page_url = base_url\n",
    "        else:\n",
    "            # Páginas 2 en adelante: usar la nueva estructura\n",
    "            # Offset: página 2 = 49 (1 + 48*1), página 3 = 97 (1 + 48*2), etc.\n",
    "            offset = 1 + (48 * i)\n",
    "            page_url = f\"{url_clean}_Desde_{offset}_NoIndex_True\"\n",
    "        \n",
    "        page_tasks.append({\n",
    "            'url': page_url,\n",
    "            'page_num': i + 1,\n",
    "            'thread_id': f\"T{(i % max_workers) + 1}\"\n",
    "        })\n",
    "        \n",
    "        # Debug: mostrar las URLs generadas\n",
    "        thread_safe_print(f\"📄 Página {i + 1}: {page_url}\")\n",
    "    \n",
    "    all_products = []\n",
    "    \n",
    "    # Ejecutar scraping con ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Enviar todas las tareas\n",
    "        future_to_page = {\n",
    "            executor.submit(scrape_single_page, page_task): page_task \n",
    "            for page_task in page_tasks\n",
    "        }\n",
    "        \n",
    "        # Procesar resultados conforme se completan\n",
    "        for future in as_completed(future_to_page):\n",
    "            page_task = future_to_page[future]\n",
    "            try:\n",
    "                page_results = future.result()\n",
    "                all_products.extend(page_results)\n",
    "                \n",
    "                # Si una página no tiene productos, podría ser el final\n",
    "                if not page_results:\n",
    "                    thread_safe_print(f\"⚠️  Página {page_task['page_num']} sin productos\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                thread_safe_print(f\"❌ Error procesando página {page_task['page_num']}: {e}\")\n",
    "    \n",
    "    thread_safe_print(f\"\\n📊 ESTADÍSTICAS FINALES:\")\n",
    "    thread_safe_print(f\"   • Páginas procesadas: {global_stats['pages_processed']}\")\n",
    "    thread_safe_print(f\"   • Productos encontrados: {global_stats['products_found']}\")\n",
    "    thread_safe_print(f\"   • Errores: {global_stats['errors']}\")\n",
    "    thread_safe_print(f\"   • Total productos únicos: {len(all_products)}\")\n",
    "    \n",
    "    return all_products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7bcc91",
   "metadata": {},
   "source": [
    "### funcion para la conección a la base de datos y main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dbfc57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_connection():\n",
    "    \"\"\"Establece conexión con SQL Server\"\"\"\n",
    "    conn_str = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};'\n",
    "    \n",
    "    # Determinar tipo de autenticación\n",
    "    if TRUSTED_CONNECTION == 'yes':\n",
    "        conn_str += 'Trusted_Connection=yes;'\n",
    "    else:\n",
    "        conn_str += f'UID={USERNAME};PWD={PASSWORD};'\n",
    "    \n",
    "    try:\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        thread_safe_print(\"Conexión a la base de datos establecida correctamente.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        thread_safe_print(f\"Error de conexión a la base de datos: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_products_table(conn):\n",
    "    \"\"\"Crea la tabla de productos si no existe\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # Verificar si la tabla existe\n",
    "        cursor.execute('''\n",
    "        IF NOT EXISTS (SELECT * FROM sys.tables WHERE name = 'ProductosMercadoLibre')\n",
    "        BEGIN\n",
    "            CREATE TABLE ProductosMercadoLibre (\n",
    "                ID INT IDENTITY(1,1) PRIMARY KEY,\n",
    "                URL NVARCHAR(MAX),\n",
    "                Nombre NVARCHAR(1000),\n",
    "                Precio INT,\n",
    "                Calificacion FLOAT,\n",
    "                Ventas INT,\n",
    "                ImagenURL NVARCHAR(MAX),\n",
    "                FechaExtraccion DATETIME DEFAULT GETDATE()\n",
    "            )\n",
    "        END\n",
    "        ELSE\n",
    "        BEGIN\n",
    "            -- Si la tabla existe, verificar y actualizar las columnas si es necesario\n",
    "            IF EXISTS (SELECT * FROM sys.columns WHERE object_id = OBJECT_ID('ProductosMercadoLibre') AND name = 'URL' AND max_length < 0)\n",
    "            BEGIN\n",
    "                PRINT 'Columnas ya tienen el tamaño correcto'\n",
    "            END\n",
    "            ELSE\n",
    "            BEGIN\n",
    "                ALTER TABLE ProductosMercadoLibre ALTER COLUMN URL NVARCHAR(MAX)\n",
    "                ALTER TABLE ProductosMercadoLibre ALTER COLUMN Nombre NVARCHAR(1000)\n",
    "                ALTER TABLE ProductosMercadoLibre ALTER COLUMN ImagenURL NVARCHAR(MAX)\n",
    "                PRINT 'Columnas actualizadas para soportar URLs más largas'\n",
    "            END\n",
    "        END\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        thread_safe_print(\"Tabla de productos verificada/creada correctamente.\")\n",
    "    except Exception as e:\n",
    "        thread_safe_print(f\"Error al crear/actualizar la tabla: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def insert_products_to_db_batch(conn, products: list[dict], batch_size: int = 100):\n",
    "    \"\"\"\n",
    "    Inserta los productos en la base de datos por lotes para mejor rendimiento.\n",
    "    \"\"\"\n",
    "    if not products:\n",
    "        thread_safe_print(\"No hay productos para insertar.\")\n",
    "        return\n",
    "    \n",
    "    total_products = len(products)\n",
    "    thread_safe_print(f\"💾 Insertando {total_products} productos en lotes de {batch_size}...\")\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        # Query de inserción\n",
    "        insert_query = '''\n",
    "        INSERT INTO ProductosMercadoLibre \n",
    "        (URL, Nombre, Precio, Calificacion, Ventas, ImagenURL)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "        '''\n",
    "        \n",
    "        # Procesar por lotes\n",
    "        for i in range(0, total_products, batch_size):\n",
    "            batch = products[i:i + batch_size]\n",
    "            \n",
    "            # Preparar los datos para la inserción\n",
    "            insert_data = []\n",
    "            for product in batch:\n",
    "                insert_data.append((\n",
    "                    product.get('url', ''),\n",
    "                    product.get('nombre', ''),\n",
    "                    product.get('precio', 0),\n",
    "                    product.get('calificacion', 0.0),\n",
    "                    product.get('ventas', 0),\n",
    "                    product.get('imagen', '')\n",
    "                ))\n",
    "            \n",
    "            # Inserción del lote\n",
    "            cursor.executemany(insert_query, insert_data)\n",
    "            conn.commit()\n",
    "            \n",
    "            progress = min(i + batch_size, total_products)\n",
    "            thread_safe_print(f\"   ✅ Lote {progress}/{total_products} productos insertados\")\n",
    "        \n",
    "        thread_safe_print(f\"🎉 {total_products} productos insertados correctamente en la base de datos.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        thread_safe_print(f\"❌ Error al insertar productos: {e}\")\n",
    "        conn.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def export_to_csv(products: list[dict], filename: str = 'productos_mercadolibre.csv'):\n",
    "    \"\"\"\n",
    "    Exporta los productos a un archivo CSV como respaldo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame(products)\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        thread_safe_print(f\"📄 Datos exportados a {filename}\")\n",
    "    except Exception as e:\n",
    "        thread_safe_print(f\"Error al exportar CSV: {e}\")\n",
    "\n",
    "def remove_duplicates(products: list[dict]) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Elimina productos duplicados basándose en la URL.\n",
    "    \"\"\"\n",
    "    seen_urls = set()\n",
    "    unique_products = []\n",
    "    \n",
    "    for product in products:\n",
    "        url = product.get('url', '')\n",
    "        if url and url not in seen_urls:\n",
    "            seen_urls.add(url)\n",
    "            unique_products.append(product)\n",
    "    \n",
    "    removed_count = len(products) - len(unique_products)\n",
    "    if removed_count > 0:\n",
    "        thread_safe_print(f\"🔄 Eliminados {removed_count} productos duplicados\")\n",
    "    \n",
    "    return unique_products\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta todo el proceso de scraping con multihilos.\n",
    "    \"\"\"\n",
    "    print(\"🚀 Iniciando scraping MULTIHILO de MercadoLibre - Componentes de PC\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Configuración\n",
    "    URL_scraping = input(\"Introduce la URL de la página que quieres scrapear: \").strip()\n",
    "    max_pages = int(input(\"¿Cuántas páginas quieres scrapear? (default: 5): \") or \"5\")\n",
    "    max_workers = int(input(f\"¿Cuántos hilos usar? (default: {MAX_WORKERS}, max recomendado: 6): \") or str(MAX_WORKERS))\n",
    "    export_csv = input(\"¿Exportar también a CSV? (s/n, default: s): \").lower() != 'n'\n",
    "    \n",
    "    # Validar configuración\n",
    "    max_workers = min(max_workers, 8)  # Límite máximo para ser respetuosos\n",
    "    \n",
    "    print(f\"\\n⚙️ CONFIGURACIÓN:\")\n",
    "    print(f\"   • Páginas a procesar: {max_pages}\")\n",
    "    print(f\"   • Hilos concurrentes: {max_workers}\")\n",
    "    print(f\"   • Delay entre requests: {REQUEST_DELAY[0]}-{REQUEST_DELAY[1]} segundos\")\n",
    "    print(f\"   • Exportar CSV: {'Sí' if export_csv else 'No'}\")\n",
    "    \n",
    "    # Resetear estadísticas globales\n",
    "    global_stats['pages_processed'] = 0\n",
    "    global_stats['products_found'] = 0\n",
    "    global_stats['errors'] = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Extraer productos con multihilos\n",
    "    print(f\"\\n📥 Iniciando extracción multihilo...\")\n",
    "    products = get_all_products_threaded(URL_scraping, max_pages, max_workers)\n",
    "    \n",
    "    if not products:\n",
    "        print(\"❌ No se pudieron extraer productos. Verifica la conexión y la URL.\")\n",
    "        return\n",
    "    \n",
    "    # 2. Eliminar duplicados\n",
    "    products = remove_duplicates(products)\n",
    "    \n",
    "    extraction_time = time.time() - start_time\n",
    "    print(f\"✅ Extracción completada en {extraction_time:.2f} segundos\")\n",
    "    print(f\"   • Productos únicos obtenidos: {len(products)}\")\n",
    "    print(f\"   • Velocidad promedio: {len(products)/extraction_time:.2f} productos/segundo\")\n",
    "    \n",
    "    # 3. Exportar a CSV (opcional)\n",
    "    if export_csv:\n",
    "        export_to_csv(products)\n",
    "    \n",
    "    # 4. Conectar a la base de datos\n",
    "    print(\"\\n🔌 Conectando a la base de datos...\")\n",
    "    conn = create_db_connection()\n",
    "    \n",
    "    if not conn:\n",
    "        print(\"❌ No se pudo conectar a la base de datos.\")\n",
    "        return\n",
    "    \n",
    "    # 5. Crear tabla si no existe\n",
    "    create_products_table(conn)\n",
    "    \n",
    "    # 6. Insertar productos por lotes\n",
    "    print(\"\\n💾 Insertando productos en la base de datos...\")\n",
    "    insert_products_to_db_batch(conn, products, batch_size=50)\n",
    "    \n",
    "    # 7. Cerrar conexión\n",
    "    conn.close()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n🎉 Proceso completado exitosamente en {total_time:.2f} segundos!\")\n",
    "    \n",
    "    # 8. Mostrar resumen final\n",
    "    print(f\"\\n📊 RESUMEN FINAL:\")\n",
    "    print(f\"   • Tiempo total: {total_time:.2f} segundos\")\n",
    "    print(f\"   • Productos extraídos: {len(products)}\")\n",
    "    print(f\"   • Páginas procesadas: {global_stats['pages_processed']}\")\n",
    "    print(f\"   • Hilos utilizados: {max_workers}\")\n",
    "    print(f\"   • Eficiencia: {len(products)/total_time:.2f} productos/segundo\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ca1373b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Iniciando scraping MULTIHILO de MercadoLibre - Componentes de PC\n",
      "======================================================================\n",
      "\n",
      "⚙️ CONFIGURACIÓN:\n",
      "   • Páginas a procesar: 2\n",
      "   • Hilos concurrentes: 4\n",
      "   • Delay entre requests: 1-2 segundos\n",
      "   • Exportar CSV: No\n",
      "\n",
      "📥 Iniciando extracción multihilo...\n",
      "🚀 Iniciando scraping multihilo con 4 workers\n",
      "📄 Página 1: https://listado.mercadolibre.com.ar/sansung-galaxy-ao4#D[A:sansung%20galaxy%20ao4,L:undefined]&origin=UNKNOWN&as.comp_t=SUG&as.comp_v=%0A&as.comp_id=HIS\n",
      "📄 Página 2: https://listado.mercadolibre.com.ar/sansung-galaxy-ao4_Desde_49_NoIndex_True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during getting browser(s): random, but was suppressed with fallback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 [Hilo T1] Procesando página 1: https://listado.mercadolibre.com.ar/sansung-galaxy-ao4#D[A:sansung%20galaxy%20ao4,L:undefined]&origin=UNKNOWN&as.comp_t=SUG&as.comp_v=%0A&as.comp_id=HIS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error occurred during getting browser(s): random, but was suppressed with fallback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 [Hilo T2] Procesando página 2: https://listado.mercadolibre.com.ar/sansung-galaxy-ao4_Desde_49_NoIndex_True\n",
      "✅ [Hilo T1] Página 1 completada: 50 productos\n",
      "✅ [Hilo T2] Página 2 completada: 50 productos\n",
      "\n",
      "📊 ESTADÍSTICAS FINALES:\n",
      "   • Páginas procesadas: 2\n",
      "   • Productos encontrados: 100\n",
      "   • Errores: 0\n",
      "   • Total productos únicos: 100\n",
      "🔄 Eliminados 2 productos duplicados\n",
      "✅ Extracción completada en 3.59 segundos\n",
      "   • Productos únicos obtenidos: 98\n",
      "   • Velocidad promedio: 27.31 productos/segundo\n",
      "\n",
      "🔌 Conectando a la base de datos...\n",
      "Conexión a la base de datos establecida correctamente.\n",
      "Tabla de productos verificada/creada correctamente.\n",
      "\n",
      "💾 Insertando productos en la base de datos...\n",
      "💾 Insertando 98 productos en lotes de 50...\n",
      "   ✅ Lote 50/98 productos insertados\n",
      "   ✅ Lote 98/98 productos insertados\n",
      "🎉 98 productos insertados correctamente en la base de datos.\n",
      "\n",
      "🎉 Proceso completado exitosamente en 3.77 segundos!\n",
      "\n",
      "📊 RESUMEN FINAL:\n",
      "   • Tiempo total: 3.77 segundos\n",
      "   • Productos extraídos: 98\n",
      "   • Páginas procesadas: 2\n",
      "   • Hilos utilizados: 4\n",
      "   • Eficiencia: 26.00 productos/segundo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
